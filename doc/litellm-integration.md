# üîå –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LiteLLM

## –û–±–∑–æ—Ä

–°–µ—Ä–≤–∏—Å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ LiteLLM –ø—Ä–æ–∫—Å–∏ –≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ OpenAI API. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç:

- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã (Anthropic, Cohere, Azure, –∏ –¥—Ä.)
- –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞—Å—Ö–æ–¥—ã –∏ –ª–∏–º–∏—Ç—ã
- –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã
- –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ LLM
- –†–∞–±–æ—Ç–∞—Ç—å –≤ —Ä–µ–≥–∏–æ–Ω–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –¥–æ—Å—Ç—É–ø–æ–º –∫ OpenAI

## –ë—ã—Å—Ç—Ä–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ LiteLLM

```bash
pip install litellm[proxy]
```

### 2. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `litellm_config.yaml`:

```yaml
model_list:
  - model_name: gpt-4-turbo-preview
    litellm_params:
      model: openai/gpt-4-turbo-preview
      api_key: sk-your-openai-key
      
  - model_name: claude-3-opus
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      api_key: sk-ant-your-anthropic-key
      
  - model_name: text-embedding-3-small
    litellm_params:
      model: openai/text-embedding-3-small
      api_key: sk-your-openai-key

litellm_settings:
  success_callback: ["langfuse"]  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
  cache: true
  cache_params:
    type: "redis"
    host: "localhost"
    port: 6379
```

### 3. –ó–∞–ø—É—Å–∫ LiteLLM

```bash
litellm --config litellm_config.yaml --port 4000
```

–ò–ª–∏ —á–µ—Ä–µ–∑ Docker:

```bash
docker run -d \
  --name litellm \
  -p 4000:4000 \
  -v $(pwd)/litellm_config.yaml:/app/config.yaml \
  ghcr.io/berriai/litellm:main-latest \
  --config /app/config.yaml --port 4000
```

### 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Ä–≤–∏—Å–∞

–û–±–Ω–æ–≤–∏—Ç–µ `.env`:

```env
# LiteLLM –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
OPENAI_API_KEY=sk-your-litellm-master-key  # –ò–ª–∏ –ª—é–±–æ–π –∫–ª—é—á –∏–∑ config
OPENAI_BASE_URL=http://localhost:4000
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```

### 5. –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞

```bash
docker-compose restart codelab-core-service
```

## –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LiteLLM
curl http://localhost:4000/health

# –°–æ–∑–¥–∞—Ç—å –∞–≥–µ–Ω—Ç–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ
curl -X POST "http://localhost:8000/my/agents/" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
    "name": "TestAgent",
    "system_prompt": "You are a helpful assistant",
    "model": "gpt-4-turbo-preview",
    ...
  }'

# –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ
curl -X POST "http://localhost:8000/my/chat/$SESSION_ID/message/" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
    "content": "Hello!",
    "target_agent": "TestAgent"
  }'
```

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

### Anthropic Claude

```yaml
model_list:
  - model_name: claude-3-opus
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      api_key: sk-ant-your-key
```

–í `.env`:
```env
OPENAI_MODEL=claude-3-opus
```

### Azure OpenAI

```yaml
model_list:
  - model_name: gpt-4-azure
    litellm_params:
      model: azure/gpt-4
      api_key: your-azure-key
      api_base: https://your-resource.openai.azure.com
      api_version: "2024-02-15-preview"
```

### Ollama (–ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏)

```yaml
model_list:
  - model_name: llama2
    litellm_params:
      model: ollama/llama2
      api_base: http://localhost:11434
```

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

### Langfuse –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

```yaml
litellm_settings:
  success_callback: ["langfuse"]
  
environment_variables:
  LANGFUSE_PUBLIC_KEY: "pk-..."
  LANGFUSE_SECRET_KEY: "sk-..."
  LANGFUSE_HOST: "https://cloud.langfuse.com"
```

### Prometheus –º–µ—Ç—Ä–∏–∫–∏

LiteLLM –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ `/metrics`:

```bash
curl http://localhost:4000/metrics
```

## –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ

### Redis –∫—ç—à

```yaml
litellm_settings:
  cache: true
  cache_params:
    type: "redis"
    host: "localhost"
    port: 6379
    ttl: 3600  # 1 —á–∞—Å
```

### In-memory –∫—ç—à

```yaml
litellm_settings:
  cache: true
  cache_params:
    type: "local"
    ttl: 600  # 10 –º–∏–Ω—É—Ç
```

## –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–¥–æ–≤

```yaml
litellm_settings:
  max_budget: 100  # $100 –≤ –º–µ—Å—è—Ü
  budget_duration: "30d"
  
model_list:
  - model_name: gpt-4-turbo-preview
    litellm_params:
      model: openai/gpt-4-turbo-preview
      api_key: sk-your-key
      max_tokens: 4096
      rpm: 60  # requests per minute
      tpm: 100000  # tokens per minute
```

## Troubleshooting

### –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ LiteLLM –∑–∞–ø—É—â–µ–Ω
curl http://localhost:4000/health

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
docker logs litellm
```

### –ù–µ–≤–µ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å

–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å —É–∫–∞–∑–∞–Ω–∞ –≤ `litellm_config.yaml` –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç `OPENAI_MODEL` –≤ `.env`.

### –û—à–∏–±–∫–∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏

–ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á–∏ –≤ `litellm_config.yaml` –∏ `OPENAI_API_KEY` –≤ `.env`.

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [LiteLLM –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](https://docs.litellm.ai/)
- [–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã](https://docs.litellm.ai/docs/providers)
- [Proxy —Å–µ—Ä–≤–µ—Ä](https://docs.litellm.ai/docs/proxy/quick_start)
